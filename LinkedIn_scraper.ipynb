from selenium.webdriver import ActionChains
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support import expected_conditions as EC
from bs4 import BeautifulSoup
import pandas as pd
import time

def combinations(name):
    string=name.upper().split(" ")
    combinations = []
    for x in range(0,len(string)):
        combinations.append(" ".join(string[:x+1]))
        return combinations
#the code that works is below, but doesn't help much:
# def combinations(name):
#     string=name.upper().split(" ")
#     combinations=[" ".join(string[:x+1]) for x in range(0,len(string)) ]
#     combinations.reverse()
#     return combinations

# Specifying n0 sandbox mode as you launch your browser[OPTIONAL]
option = webdriver.ChromeOptions()
option.add_argument("--no-sandbox")

browser = webdriver.Chrome(r'''C:\Users\tisch\chromedriver_win32\chromedriver.exe''', chrome_options=option)

# Go to desired website
browser.get('https://www.linkedin.com/')

# Wait 20 seconds for page to load
browser.implicitly_wait(5)
timeout = 20

# Get all of the titles for the pinned repositories
# We are not just getting pure titles but we are getting a selenium object
# with selenium elements of the titles.

elem = browser.find_element_by_name('session_key')
elem.clear()
elem.send_keys("")  # your email id
elem = browser.find_element_by_name('session_password')
elem.clear()
elem.send_keys("")  # your password

# find_elements_by_xpath - Returns an array of selenium objects.
submit = browser.find_element_by_xpath("//*[@type='submit']")

# List Comprehension to get the actual repo titles and not the selenium objects.
# Button accessed using the inspecting feature of Chrome and then copying the Xpath of the button to be clicked

actions = ActionChains(browser)
actions.click(submit)
actions.perform()
# df = pd.read_csv("competitors.csv")
# companies = df["Company"]
# companies = companies.dropna()
companies = ['Microsoft', 'Cognism', 'Charie Contacts', 'Ignite Selling', 'Scribe', 'Prospect.io']

names = []
websites = []
headquarters = []
yearfoundeds = []
companytypes = []
sizes = []
specialitys = []
information = []

for company in list(companies):
    companies_name=combinations(company)
    print(company)
    for name in companies_name:
        try:
            company_s = company.lower().replace(" ", "-")#.replace(".", "-")
            browser.get('https://in.linkedin.com/company/' + company_s + "/")
            print('https://in.linkedin.com/company/' + company_s + "/")
            time.sleep(10)
            soup = BeautifulSoup(browser.page_source, 'lxml')
            text = soup.text.replace("\n", "$")

            text = text.replace("  ", "")
            text = text.replace("$$", "$")
            string = text[text.find("Company details") + len("Company details") + 1: text.find("See more details about")]
            string = [s.strip() for s in string.split("$") if s.strip()]

            dictionary = {}
            for itr in range(0, len(string) - 1, 2):
                dictionary[string[itr]] = string[itr + 1]
            data = []
            data.append(company)
            main = {
                "Website": 0,
                "Headquarters": 0,
                "Company size": 0,
                "Year founded": 0,
                "Specialties": 0
            }
            for key in dictionary.keys():
                if key in main:
                    main[key] = dictionary[key].replace("employees", "").strip()
            for value in main.values():
                data.append(value)

            information.append(data)
        except:
            pass
    
#     records = []  
# for result in results:  
#     date = result.find('strong').text[0:-1] + ', 2017'
#     lie = result.contents[1][1:-2]
#     explanation = result.find('a').text[1:-1]
#     url = result.find('a')['href']
#     records.append((company, website, headquarters, company size))

print(information)

df = pd.DataFrame(information, columns=[data, 'company', 'website', 'headquarters', 'company size', 'specialties'])
df.to_csv('competitors_scraped_test.csv', index=False, encoding='utf-8') 
